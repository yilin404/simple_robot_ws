# @package _global_

seed: 100000
dataset_repo_id: yilin404/pick_and_place
dataset_root: /home/yilin/dataset/own_episode_data
override_dataset_stats: 
  action: 
    max: [2.8798,  3.1415, 0,       2.8798,  1.6581,  2.8798,  0.03]
    min: [-2.8798, 0,      -3.3161, -2.8798, -1.6581, -2.8798, 0.]
  observation.state:
    max: [2.8798,  3.1415, 0,       2.8798,  1.6581,  2.8798,  0.03]
    min: [-2.8798, 0,      -3.3161, -2.8798, -1.6581, -2.8798, 0.]
  observation.images.colors_camera_top:
    # stats from imagenet, since we use a pretrained vision model
    mean: [[[0.485]], [[0.456]], [[0.406]]]  # (c,1,1)
    std: [[[0.229]], [[0.224]], [[0.225]]]  # (c,1,1)
  observation.images.colors_camera_wrist:
    # stats from imagenet, since we use a pretrained vision model
    mean: [[[0.485]], [[0.456]], [[0.406]]]  # (c,1,1)
    std: [[[0.229]], [[0.224]], [[0.225]]]  # (c,1,1)
  observation.images.colors_camera_right:
    # stats from imagenet, since we use a pretrained vision model
    mean: [[[0.485]], [[0.456]], [[0.406]]]  # (c,1,1)
    std: [[[0.229]], [[0.224]], [[0.225]]]  # (c,1,1)

training:
  offline_steps: 80000
  online_steps: 0
  eval_freq: -1
  save_freq: 500 
  log_freq: 100
  save_checkpoint: True

  batch_size: 8
  num_workers: 4
  grad_clip_norm: 10
  lr: 1.0e-4
  lr_scheduler: cosine
  lr_warmup_steps: 500
  adam_betas: [0.95, 0.999]
  adam_eps: 1.0e-8
  adam_weight_decay: 1.0e-6

  delta_timestamps:
    observation.images.colors_camera_top: "[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1)]"
    observation.images.colors_camera_wrist: "[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1)]"
    observation.images.colors_camera_right: "[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1)]"
    observation.state: "[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1)]"
    action: "[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1 - ${policy.n_obs_steps} + ${policy.horizon})]"

  # The original implementation doesn't sample frames for the last 7 steps,
  # which avoids excessive padding and leads to improved training results.
  drop_n_last_frames: 24  # ${policy.horizon} - ${policy.n_action_steps} - ${policy.n_obs_steps} + 1

  image_transforms:
  # These transforms are all using standard torchvision.transforms.v2
  # You can find out how these transformations affect images here:
  # https://pytorch.org/vision/0.18/auto_examples/transforms/plot_transforms_illustrations.html
  # We use a custom RandomSubsetApply container to sample them.
  # For each transform, the following parameters are available:
  #   weight: This represents the multinomial probability (with no replacement)
  #           used for sampling the transform. If the sum of the weights is not 1,
  #           they will be normalized.
  #   min_max: Lower & upper bound respectively used for sampling the transform's parameter
  #           (following uniform distribution) when it's applied.
    # Set this flag to `True` to enable transforms during training
    enable: True
    # This is the maximum number of transforms (sampled from these below) that will be applied to each frame.
    # It's an integer in the interval [1, number of available transforms].
    max_num_transforms: 3
    # By default, transforms are applied in Torchvision's suggested order (shown below).
    # Set this to True to apply them in a random order.
    random_order: False
    brightness:
      weight: 1
      min_max: [0.8, 1.2]
    contrast:
      weight: 1
      min_max: [0.8, 1.2]
    saturation:
      weight: 1
      min_max: [0.5, 1.5]
    hue:
      weight: 1
      min_max: [-0.05, 0.05]
    sharpness:
      weight: 1
      min_max: [0.8, 1.2]

eval:
  n_episodes: 50
  batch_size: 50

policy:
  name: rdt

  # Input / output structure.
  n_obs_steps: 1
  horizon: 48
  n_action_steps: 24

  input_shapes:
    # TODO(rcadene, alexander-soare): add variables for height and width from the dataset/env?
    observation.images.colors_camera_top: [3, 480, 640]
    observation.images.colors_camera_wrist: [3, 480, 640]
    observation.images.colors_camera_right: [3, 480, 640]
    observation.state: ["${env.state_dim}"]
  output_shapes:
    action: ["${env.action_dim}"]

  # Normalization / Unnormalization
  input_normalization_modes:
    observation.images.colors_camera_top: mean_std
    observation.images.colors_camera_wrist: mean_std
    observation.images.colors_camera_right: mean_std
    observation.state: min_max
  output_normalization_modes:
    action: min_max

  # Architecture / modeling.
  # Vision backbone.
  vision_backbone: resnet18
  crop_shape: null
  crop_is_random: False
  pretrained_backbone_weights: ResNet18_Weights.IMAGENET1K_V1
  use_group_norm: False
  # Unet.
  condition_embed_dim: 256
  down_dims: [256, 512, 1024]
  kernel_size: 5
  n_groups: 8
  num_heads: 8
  dropout: 0.1
  # Noise scheduler.
  noise_scheduler_type: DDIM
  num_train_timesteps: 100
  beta_schedule: squaredcos_cap_v2
  beta_start: 0.0001
  beta_end: 0.02
  prediction_type: sample # epsilon / sample
  clip_sample: True
  clip_sample_range: 1.0

  # Inference
  num_inference_steps: 10  # if not provided, defaults to `num_train_timesteps`

  # Loss computation
  do_mask_loss_for_padding: True
