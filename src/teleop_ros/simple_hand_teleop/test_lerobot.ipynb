{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "\n",
    "REPO_ID = \"yilin404/pick_and_place\"\n",
    "DATA_ROOT = \"/home/yilin/dataset/own_episode_data\"\n",
    "\n",
    "# Set up the dataset.\n",
    "delta_timestamps = {\n",
    "    # Load the previous image and state at -0.1 seconds before current frame,\n",
    "    # then load current image and state corresponding to 0.0 second.\n",
    "    \"observation.images.colors_camera_top\": [-0.1, 0.0],\n",
    "    \"observation.images.colors_camera_wrist\": [-0.1, 0.0],\n",
    "    \"observation.state\": [-0.1, 0.0],\n",
    "    # Load the previous action (-0.1), the next action to be executed (0.0),\n",
    "    # and 14 future actions with a 0.1 seconds spacing. All these actions will be\n",
    "    # used to supervise the policy.\n",
    "    \"action\": [-0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4],\n",
    "}\n",
    "dataset = LeRobotDataset(REPO_ID, root=DATA_ROOT, delta_timestamps=delta_timestamps)\n",
    "\n",
    "print(\"===> logging dataset info...\")\n",
    "for key, value in dataset[0].items():\n",
    "    print(\"key name is: \", key)\n",
    "    if key in delta_timestamps.keys():\n",
    "        print(value.size(), type(value))\n",
    "print(\"===> logging dataset info...\\n\")\n",
    "\n",
    "print(\"===> logging dataset stats...\")\n",
    "print(dataset.stats)\n",
    "print(\"===> logging dataset stats...\\n\")\n",
    "\n",
    "# Create dataloader for offline training.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=0,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    pin_memory=device != torch.device(\"cpu\"),\n",
    "    drop_last=True,\n",
    ")\n",
    "for batch in dataloader:\n",
    "    batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "    print(\"===> logging dataloader info...\")\n",
    "    for key, value in batch.items():\n",
    "        print(\"key name is: \", key)\n",
    "        if key in delta_timestamps.keys():\n",
    "            print(value.size(), type(value))\n",
    "    print(\"===> logging dataloader info...\\n\")\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 episodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 60 episodes: 100%|██████████| 60/60 [00:01<00:00, 35.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import List\n",
    "\n",
    "RAW_DIR = Path(\"/home/yilin/dataset/own_episode_data/raw_data/pick_and_place\")\n",
    "JSON_FILE = \"data.json\"\n",
    "\n",
    "def get_episodes(raw_dir: Path) -> List[Path]:\n",
    "    return [path for path in raw_dir.iterdir() if path.is_dir()]\n",
    "\n",
    "episode_paths = get_episodes(RAW_DIR)\n",
    "print(f\"Found {len(episode_paths)} episodes.\")\n",
    "\n",
    "episode_paths = sorted(\n",
    "    episode_paths,\n",
    "    key=lambda path: int(re.search(r'(\\d+)$', str(path)).group(1)) if re.search(r'(\\d+)$', str(path)) else -1\n",
    ")\n",
    "num_episodes = len(episode_paths)\n",
    "\n",
    "for ep_path in tqdm.tqdm(episode_paths, desc=f\"Processing {num_episodes} episodes\"):\n",
    "    json_path = ep_path / JSON_FILE\n",
    "    if not json_path.exists():\n",
    "        print(f\"Warning: {json_path} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with json_path.open('r+', encoding='utf-8') as jsonf:\n",
    "            # 加载 JSON 文件\n",
    "            episode_data = json.load(jsonf)\n",
    "\n",
    "            # 修改数据\n",
    "            for sample_data in episode_data.get(\"data\", []):\n",
    "                arm_states = np.array(sample_data[\"states\"].get(\"arm\", {})[\"qpos\"], dtype=np.float32)\n",
    "                arm_actions = np.array(sample_data[\"actions\"].get(\"arm\", {})[\"qpos\"], dtype=np.float32)\n",
    "\n",
    "                arm_actions = arm_states + np.clip(arm_actions - arm_states, a_min=-0.1, a_max=0.1)\n",
    "\n",
    "                sample_data[\"actions\"][\"arm\"][\"qpos\"] = arm_actions.tolist()\n",
    "            \n",
    "            # 写入修改后的数据\n",
    "            jsonf.seek(0)  # 将文件指针移动到文件开头\n",
    "            jsonf.truncate()  # 清空文件内容\n",
    "            json.dump(episode_data, jsonf, indent=4, ensure_ascii=False)  # 写入修改后的数据\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON in {json_path}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
